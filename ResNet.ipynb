{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77163f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 13s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29084464/29084464 [==============================] - 5s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import resnet, densenet, vgg16, vgg19\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess_input\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess_input\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "resnet_model = resnet.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Load pre-trained DenseNet121 model\n",
    "densenet_model = densenet.DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "vgg_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e78cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 246ms/step\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "# Preprocess input images for each model\n",
    "def preprocess_image(img_path, model_name):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    if model_name == 'resnet':\n",
    "        img_array = resnet_preprocess_input(img_array)\n",
    "    elif model_name == 'densenet':\n",
    "        img_array = densenet_preprocess_input(img_array)\n",
    "    elif model_name == 'vgg':\n",
    "        img_array = vgg_preprocess_input(img_array)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Example usage\n",
    "for i in range(10):\n",
    "    img_path = 'D:/Dersler-20210512T064119Z-001/Dersler/2023_Fall/COMP411/COMP411-FineART/toy_dataset/toy_dataset/{}.jpg'.format(i+1)\n",
    "    resnet_input = preprocess_image(img_path, 'resnet')\n",
    "    resnet_features = resnet_model.predict(resnet_input)\n",
    "    train_dataset.append(resnet_features)\n",
    "\n",
    "    \n",
    "densenet_input = preprocess_image(img_path, 'densenet')\n",
    "vgg_input = preprocess_image(img_path, 'vgg')\n",
    "\n",
    "# Extract features from pre-trained models\n",
    "resnet_features = resnet_model.predict(resnet_input)\n",
    "densenet_features = densenet_model.predict(densenet_input)\n",
    "vgg_features = vgg_model.predict(vgg_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb292dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    1.4155909 ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    1.6893629 ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.01841718\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.06271434 0.         ... 0.         0.\n",
      "    0.25866926]\n",
      "   [0.         0.0669893  0.         ... 0.         0.\n",
      "    2.0966072 ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.48695505]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         2.1098237\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.03330207 1.3731451\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.23826277\n",
      "    0.        ]\n",
      "   [0.         0.24416709 0.         ... 0.         0.8383591\n",
      "    3.3056755 ]\n",
      "   [0.         0.         0.         ... 0.         0.04921114\n",
      "    1.1970228 ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         4.204914\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.44198835 7.244743\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 2.1058216  4.269782\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.06300151 0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.40123892 0.         ... 0.         0.\n",
      "    0.7417799 ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    1.1360135 ]\n",
      "   ...\n",
      "   [0.         0.3176527  0.         ... 0.         0.04574633\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 1.8381523  1.876134\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.7204149  1.0143665\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.3136698 ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         1.6317995\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.973862\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.01340246 0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(resnet_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f479068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "print(len(base_model.layers))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "toy_data_path = 'toy_dataset_label.csv'\n",
    "toy_data = pd.read_csv(toy_data_path, on_bad_lines='skip', delimiter='\\t',quoting=csv.QUOTE_NONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5be7f264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mythological']\n",
      " ['mythological']\n",
      " ['genre']\n",
      " ['portrait']\n",
      " ['portrait']\n",
      " ['landscape']\n",
      " ['landscape']\n",
      " ['religious']\n",
      " ['other']\n",
      " ['other']]\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"sequential_8\" expects 1 input(s), but it received 10 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 7, 7, 2048) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([elm]))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(y))\n\u001b[1;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileuha_qs_7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"sequential_8\" expects 1 input(s), but it received 10 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 7, 7, 2048) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 7, 7, 2048) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "labels_toy_data = toy_data.iloc[:, 9].values[:10]\n",
    "\n",
    "a = set()\n",
    "for val in labels_toy_data:\n",
    "    if pd.notna(val):\n",
    "        a.add(val)\n",
    "num_classes = len(a)\n",
    "\n",
    "\"\"\"# Freeze some layers for fine-tuning\n",
    "for layer in base_model.layers[:170]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new layers for your specific task\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Adjust num_classes based on your task\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "y= []\n",
    "for elm in labels_toy_data:\n",
    "    y.append(np.array([elm]))\n",
    "print(np.array(y))\n",
    "model.fit(x=train_dataset, y=y, epochs=5)\"\"\"\n",
    "\n",
    "#########################################\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_aug_transforms = transforms.Compose([transforms.RandomCrop(500, pad_if_needed=True), transforms.ToTensor()])\n",
    "\n",
    "def load_dataset():\n",
    "    data_path = '/src/wikiart'\n",
    "    #data_path = '/content/wikitest'\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=data_aug_transforms\n",
    "    )\n",
    "    #train_mask = list(range(num_training))\n",
    "    length = len(dataset)\n",
    "    num_training= int(0.6 * length)\n",
    "    num_validation = int(0.2 * length)\n",
    "    num_test = length - num_training - num_validation\n",
    "    lengths = [num_training, num_validation, num_test] \n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "    #print(len(train_dataset))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    #val_mask = list(range(num_training, num_training + num_validation))\n",
    "    #val_dataset = torch.utils.data.Subset(dataset, mask)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have a dataset class, let's call it CustomDataset, with images and labels\n",
    "\n",
    "# Define transforms for training data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create your custom dataset instance\n",
    "dataset = CustomDataset(root='path/to/your/images', transform=transform)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load the pre-trained ResNet model\n",
    "resnet_model = models.resnet18(pretrained=True)\n",
    "# Change the output layer to match the number of classes in your dataset\n",
    "num_classes = len(dataset.classes)\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, num_classes)\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    resnet_model.train()\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "resnet_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "        outputs = resnet_model(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Now, 'all_predictions' contains the predicted labels for the test set\n",
    "\n",
    "# You can use the trained model to predict labels for the rest of your images in a similar way"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
