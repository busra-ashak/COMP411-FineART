{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22117f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\busr4\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27275489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(20),      # Rotate the image by up to 20 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly change brightness, contrast, and saturation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = torch.from_numpy(labels).type(torch.LongTensor)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df74ac51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\busr4/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 548M/548M [00:50<00:00, 11.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Load pre-trained models\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "toy_data_path = 'toy_dataset_label.csv'\n",
    "toy_data = pd.read_csv(toy_data_path, on_bad_lines='skip', delimiter='\\t',quoting=csv.QUOTE_NONE,encoding='unicode_escape')\n",
    "labels_toy_data = toy_data.iloc[:, 9].values\n",
    "\n",
    "classes = set()\n",
    "for val in labels_toy_data:\n",
    "    if pd.notna(val):\n",
    "        classes.add(val)\n",
    "num_classes = len(classes)\n",
    "vgg19.classifier[6] = torch.nn.Linear(vgg19.classifier[6].in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "533c8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.weight, Frozen: True\n",
      "Layer: features.0.bias, Frozen: True\n",
      "Layer: features.2.weight, Frozen: True\n",
      "Layer: features.2.bias, Frozen: True\n",
      "Layer: features.5.weight, Frozen: True\n",
      "Layer: features.5.bias, Frozen: True\n",
      "Layer: features.7.weight, Frozen: True\n",
      "Layer: features.7.bias, Frozen: True\n",
      "Layer: features.10.weight, Frozen: True\n",
      "Layer: features.10.bias, Frozen: True\n",
      "Layer: features.12.weight, Frozen: True\n",
      "Layer: features.12.bias, Frozen: True\n",
      "Layer: features.14.weight, Frozen: True\n",
      "Layer: features.14.bias, Frozen: True\n",
      "Layer: features.16.weight, Frozen: True\n",
      "Layer: features.16.bias, Frozen: True\n",
      "Layer: features.19.weight, Frozen: True\n",
      "Layer: features.19.bias, Frozen: True\n",
      "Layer: features.21.weight, Frozen: True\n",
      "Layer: features.21.bias, Frozen: True\n",
      "Layer: features.23.weight, Frozen: True\n",
      "Layer: features.23.bias, Frozen: True\n",
      "Layer: features.25.weight, Frozen: True\n",
      "Layer: features.25.bias, Frozen: True\n",
      "Layer: features.28.weight, Frozen: True\n",
      "Layer: features.28.bias, Frozen: True\n",
      "Layer: features.30.weight, Frozen: True\n",
      "Layer: features.30.bias, Frozen: True\n",
      "Layer: features.32.weight, Frozen: True\n",
      "Layer: features.32.bias, Frozen: True\n",
      "Layer: features.34.weight, Frozen: False\n",
      "Layer: features.34.bias, Frozen: False\n",
      "Layer: classifier.0.weight, Frozen: False\n",
      "Layer: classifier.0.bias, Frozen: False\n",
      "Layer: classifier.3.weight, Frozen: False\n",
      "Layer: classifier.3.bias, Frozen: False\n",
      "Layer: classifier.6.weight, Frozen: False\n",
      "Layer: classifier.6.bias, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "#freezes up to this layer, non-inclusive\n",
    "freeze_until_layer = 'features.32'\n",
    "\n",
    "# Flag to mark when to start unfreezing layers\n",
    "freeze = True\n",
    "\n",
    "def freeze_layers(model, freeze_until_layer):\n",
    "    freeze = True\n",
    "    for name, module in model.named_modules():\n",
    "        if name == freeze_until_layer:\n",
    "            freeze = False\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = not freeze\n",
    "    return model\n",
    "\n",
    "vgg19 = freeze_layers(vgg19, freeze_until_layer)\n",
    "            \n",
    "\n",
    "\"\"\"# You can print out to check which layers are frozen and which are not\n",
    "for name, param in vgg19.named_parameters():\n",
    "    print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ffad9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training finished')\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                outputs = model(val_images)\n",
    "                loss = criterion(outputs, val_labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                total_samples += val_labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = total_correct / total_samples\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        average_loss = test_loss / len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        print(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01804864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training finished\n",
      "Epoch [1/5], Validation Loss: 1.7119, Validation Accuracy: 0.4750\n",
      "Epoch [2/5], Training finished\n",
      "Epoch [2/5], Validation Loss: 1.4145, Validation Accuracy: 0.5750\n",
      "Epoch [3/5], Training finished\n",
      "Epoch [3/5], Validation Loss: 1.1946, Validation Accuracy: 0.6250\n",
      "Epoch [4/5], Training finished\n",
      "Epoch [4/5], Validation Loss: 1.1147, Validation Accuracy: 0.6500\n",
      "Epoch [5/5], Training finished\n",
      "Epoch [5/5], Validation Loss: 1.0200, Validation Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "all_image_paths = []\n",
    "for i in range(43455):\n",
    "    if(i != 21911 and i != 19561):\n",
    "        img_path = 'toy_dataset/toy_dataset/{}.jpg'.format(i+1)\n",
    "        all_image_paths.append(img_path)\n",
    "    \n",
    "all_labels = labels_toy_data[:8691]\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_paths, label_encoder.fit_transform(train_labels), transform=train_transform)\n",
    "val_dataset = CustomDataset(val_paths, label_encoder.fit_transform(val_labels), transform=test_transform)\n",
    "test_dataset = CustomDataset(test_paths, label_encoder.fit_transform(test_labels), transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "trained_vgg19 = train_model(vgg19, train_loader, val_loader, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f77f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.8608, Test Accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "test_model(trained_vgg19, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eef624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
